---
layout: default
---

## About Me

<img class="profile-picture" src="self.jpeg">

Hi! I'm a Research Scientist at [Orby AI](http://orby.ai/).

## Research Interest

I have been focusing on multimodal LLM post-training recently, specifically multi-turn RL for GUI agents. In the past, I worked on multimodal LLMs for GUI understanding and user modeling.

## Selected Publications

1. **Gang Li** and Yang Li. [Spotlight: Mobile UI understanding using vision-language models with a focus](https://arxiv.org/abs/2209.14927). In
The Eleventh International Conference on Learning Representations, 2023. [[Google Research Blog](https://research.google/blog/a-vision-language-approach-for-foundational-ui-understanding/)].
2. Youwei Liang*, Junfeng He*, **Gang Li***, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi
Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo,
Yang Li, Kai J Kohlhoff, Deepak Ramachandran, and Vidhya Navalpakkam. [Rich human feedback for
text-to-image generation](https://arxiv.org/abs/2312.10240). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2024. * Co-first authors. Best Paper Award. [[Google Research Blog](https://research.google/blog/rich-human-feedback-for-text-to-image-generation/)].
3. Bryan Wang, **Gang Li**, and Yang Li. [Enabling conversational interaction with mobile UI using large language
models](https://arxiv.org/abs/2209.08655). In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1â€“17, 2023. [[Google Research Blog](https://research.google/blog/enabling-conversational-interaction-on-mobile-with-llms/)].

The full list can be found on my Google Scholar page.